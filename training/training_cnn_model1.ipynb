{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details about implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Model is implemented from a YouTube Tutorial, Its simple and efficient. This notebook includes all the necessary code and visualization that may help understand the model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"plant_village_cnn_model1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the dataset folders\n",
    "TRAIN_DIR = \"../datasets/cropped_plant_village_dataset/train\"\n",
    "VALID_DIR = \"../datasets/cropped_plant_village_dataset/valid\"\n",
    "SAMPLE_IMAGE = \"../datasets/cropped_plant_village_dataset/sample_image.JPG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    VALID_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count images in each class\n",
    "def count_images_in_classes(dataset_dir):\n",
    "    \"\"\"\n",
    "    Counts the number of images in each class within a dataset directory.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The path to the dataset directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the class names and the values are the number of images in each class.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_counts[class_name] = len(os.listdir(class_path))\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "# Count images in training and validation sets\n",
    "train_class_counts = count_images_in_classes(TRAIN_DIR)\n",
    "valid_class_counts = count_images_in_classes(VALID_DIR)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": list(train_class_counts.keys()),\n",
    "        \"Training Images\": list(train_class_counts.values()),\n",
    "        \"Validation Images\": [\n",
    "            valid_class_counts.get(cls, 0) for cls in train_class_counts.keys()\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class distribution\n",
    "df.plot(\n",
    "    x=\"Class\", kind=\"bar\", stacked=True, figsize=(15, 6), title=\"Class Distribution\"\n",
    ")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize one image per class\n",
    "def visualize_sample_images(dataset_dir):\n",
    "    \"\"\"\n",
    "    Visualizes one sample image per class in the dataset directory.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): The path to the dataset directory.\n",
    "\n",
    "    Displays a grid of images, with one image per class, using matplotlib.\n",
    "    \"\"\"\n",
    "    class_names = os.listdir(dataset_dir)\n",
    "    class_names.sort()  # Sort for consistent order\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, class_name in enumerate(class_names, start=1):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        image_path = os.path.join(\n",
    "            class_path, os.listdir(class_path)[0]\n",
    "        )  # Get the first image in the class\n",
    "        img = plt.imread(image_path)\n",
    "\n",
    "        plt.subplot(4, 5, i)  # Adjust grid size for the number of classes\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_name)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from training set\n",
    "visualize_sample_images(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Apply normalization on both Training and Validation set\n",
    "training_set = training_set.map(lambda x, y: (normalize(x), y))\n",
    "validation_set = validation_set.map(lambda x, y: (normalize(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential(\n",
    "    [\n",
    "        # Brightness adjustment\n",
    "        tf.keras.layers.RandomBrightness(factor=0.2),  # Adjust brightness by ±20%\n",
    "\n",
    "        # Contrast adjustment\n",
    "        tf.keras.layers.RandomContrast(factor=0.2),  # Adjust contrast by ±20%\n",
    "\n",
    "        # Rotation\n",
    "        tf.keras.layers.RandomRotation(factor=0.1),  # Rotate by ±10% (36°)\n",
    "        \n",
    "        # Horizontal and vertical flips\n",
    "        tf.keras.layers.RandomFlip(\n",
    "            mode=\"horizontal_and_vertical\"\n",
    "        ),  # Flip both horizontally and vertically\n",
    "        \n",
    "        # Zoom\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)\n",
    "        ),  # Zoom in/out by 20%\n",
    "\n",
    "        # Gaussian noise\n",
    "        tf.keras.layers.Lambda(\n",
    "            lambda x: x + tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.01)\n",
    "        ),  # Add Gaussian noise\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply augmentation to the training set\n",
    "augmented_training_set = training_set.map(lambda x, y: (data_augmentation(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Enhancement (not implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Enhancements have not strongly proven to increase the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Preprocessing Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Augmentation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the augmentation pipeline with individual augmentations\n",
    "def visualize_individual_augmentations(image_path):\n",
    "    \"\"\"\n",
    "    Visualizes the effect of individual augmentations on an input image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the input image.\n",
    "\n",
    "    Applies a series of individual augmentations to the input image and displays the results in a grid.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=(128, 128))  # Adjust to your image size\n",
    "    image_array = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "    image_array = tf.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Define individual augmentation layers\n",
    "    augmentations = [\n",
    "        (\"Original\", None),\n",
    "        (\"Random Brightness\", tf.keras.layers.RandomBrightness(factor=0.2)),\n",
    "        (\"Random Contrast\", tf.keras.layers.RandomContrast(factor=0.2)),\n",
    "        (\"Random Rotation\", tf.keras.layers.RandomRotation(factor=0.1)),\n",
    "        (\"Random Flip\", tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")),\n",
    "        (\n",
    "            \"Random Zoom\",\n",
    "            tf.keras.layers.RandomZoom(\n",
    "                height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"Gaussian Noise\",\n",
    "            tf.keras.layers.Lambda(\n",
    "                lambda x: x + tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=0.01)\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Apply each augmentation and plot\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i, (title, layer) in enumerate(augmentations, start=1):\n",
    "        if layer is None:\n",
    "            augmented_image = image_array[0]\n",
    "        else:\n",
    "            augmented_image = layer(image_array)[0]\n",
    "\n",
    "        plt.subplot(1, len(augmentations), i)\n",
    "        plt.imshow(augmented_image.numpy())\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual augmentation techniques\n",
    "visualize_individual_augmentations(SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the augmentation pipeline with a sample image\n",
    "def visualize_augmentation(image_path):\n",
    "    \"\"\"\n",
    "    Visualizes the effect of the augmentation pipeline on a sample image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the input image.\n",
    "\n",
    "    Applies the augmentation pipeline to the input image and displays the original image alongside 5 augmented versions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=(128, 128))  # Adjust to your image size\n",
    "    image_array = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "    image_array = tf.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Apply augmentations\n",
    "    augmented_images = [data_augmentation(image_array)[0] for _ in range(5)]\n",
    "\n",
    "    # Plot original and augmented images\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(image_array[0])\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i, aug_img in enumerate(augmented_images, start=2):\n",
    "        plt.subplot(1, 6, i)\n",
    "        plt.imshow(aug_img.numpy())\n",
    "        plt.title(f\"Augmented {i-1}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Original vs Augmented Image\n",
    "visualize_augmentation(SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_block(\n",
    "    model,\n",
    "    filters,\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    strides=2,\n",
    "    activation=\"relu\",\n",
    "    padding=\"same\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Adds a convolutional block to the model, consisting of:\n",
    "    - Conv2D layer\n",
    "    - Conv2D layer\n",
    "    - MaxPooling layer\n",
    "\n",
    "    Parameters:\n",
    "        model: Sequential model to which the block is added.\n",
    "        filters: Number of filters for Conv2D layers.\n",
    "        kernel_size: Size of the convolutional kernel (default: 3).\n",
    "        pool_size: Pool size for MaxPooling (default: 2).\n",
    "        strides: Strides for MaxPooling (default: 2).\n",
    "        activation: Activation function for Conv2D layers (default: 'relu').\n",
    "        padding: Padding for Conv2D layers (default: 'same').\n",
    "    \"\"\"\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            activation=activation,\n",
    "        )\n",
    "    )\n",
    "    model.add(Conv2D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "    model.add(MaxPool2D(pool_size=pool_size, strides=strides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer and first convolutional block\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=[128, 128, 3],\n",
    "    )\n",
    ")\n",
    "model.add(Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subsequent convolutional blocks using the function\n",
    "add_conv_block(model, filters=64)\n",
    "add_conv_block(model, filters=128)\n",
    "add_conv_block(model, filters=256)\n",
    "add_conv_block(model, filters=512)\n",
    "add_conv_block(model, filters=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the fully connected layers\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1500, activation=\"relu\"))\n",
    "model.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Layer\n",
    "model.add(Dense(units=38, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = model.fit(x=training_set, validation_data=validation_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set Accuracy\n",
    "train_loss, train_acc = model.evaluate(training_set)\n",
    "print(\"Training accuracy:\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set Accuracy\n",
    "val_loss, val_acc = model.evaluate(validation_set)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'../models/{MODEL_NAME}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "with open(\"training_history.json\", \"w\") as f:\n",
    "    json.dump(training_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class indices to class names\n",
    "class_names = validation_set.class_names\n",
    "print(f\"Class Names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true labels\n",
    "y_true = np.concatenate([y.numpy() for _, y in validation_set], axis=0)\n",
    "y_true = np.argmax(y_true, axis=1)  # Convert one-hot encoding to class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the trained model\n",
    "y_pred_probs = model.predict(validation_set)  # Replace 'model' with your trained model\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, validation_set, class_names):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix for a given model and validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model.\n",
    "        validation_set: Validation dataset (normalized and preprocessed).\n",
    "        class_names: List of class labels.\n",
    "    \"\"\"\n",
    "    # Get true labels and predictions\n",
    "    true_labels = np.concatenate([y for x, y in validation_set], axis=0)\n",
    "    predicted_probs = model.predict(validation_set)\n",
    "    predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "    true_labels = np.argmax(true_labels, axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "class_names = validation_set.class_names  # Assuming your dataset has class names\n",
    "plot_confusion_matrix(model, validation_set, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train | Valid Accuracy & Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracy and loss graphs.\n",
    "\n",
    "    Parameters:\n",
    "        history: The History object returned by model.fit().\n",
    "    \"\"\"\n",
    "    # Extract metrics\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs, val_acc, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graphs\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive template for training a CNN model for image classification. It includes dataset preparation, preprocessing, augmentation, model building, training, evaluation, and visualization. Once executed, it is expected to deliver a functional model for plant disease classification. However, adding hyperparameter tuning and overfitting mitigation strategies could further improve its utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Areas for Improvement:\n",
    "\n",
    "The notebook lacks details about hyperparameter tuning. Including experiments with different learning rates or optimizers could enhance it.\n",
    "While the augmentation techniques are detailed, their effectiveness on model performance is not quantified.\n",
    "Consider adding a section on overfitting prevention strategies like early stopping or learning rate scheduling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".AgroDiagnoseAI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
