{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning EfficientV2B3 Raw Data i224 b32 e30 ft2e20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the EfficientV2B3 model will be trained on the Raw Dataset. \n",
    "\n",
    "Image size is 224, Batch size is 32, Epochs is 30, model is trained then finetuned again on the same dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 22:53:52.578701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738364032.701332   32905 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738364032.735534   32905 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "from tensorflow.keras.applications import InceptionV3, EfficientNetV2B3\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "MODEL_NAME = \"tl_efficientv2b3_raw_i224_b32_e30_ft2e20\"\n",
    "\n",
    "NUM_CLASSES = 72\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "# Get the base directory\n",
    "BASE_DIR = Path.cwd().parent.parent\n",
    "\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /home/sam5io/sam_engineerings/AgroDiagnoseAI/datasets/plant_disease_dataset_noaugmentation_raw\n",
      "Sample Image Path: /home/sam5io/sam_engineerings/AgroDiagnoseAI/datasets/cropped_plant_village_dataset/sample_image.JPG\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = BASE_DIR / \"datasets\" / \"plant_disease_dataset_noaugmentation_raw\"\n",
    "SAMPLE_IMAGE = (\n",
    "    BASE_DIR / \"datasets\" / \"cropped_plant_village_dataset\" / \"sample_image.JPG\"\n",
    ")\n",
    "\n",
    "print(\"Directory:\", DATA_DIR)\n",
    "print(\"Sample Image Path:\", SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116232 files belonging to 72 classes.\n",
      "Using 92986 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738364419.566969   32905 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m      2\u001b[0m training_set \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[1;32m      3\u001b[0m     DATA_DIR,\n\u001b[1;32m      4\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     crop_to_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load validation dataset\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m validation_set \u001b[38;5;241m=\u001b[39m \u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrop_to_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:232\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/keras/src/utils/dataset_utils.py:578\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    576\u001b[0m labels_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m--> 578\u001b[0m     partial_filenames, partial_labels \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     labels_list\u001b[38;5;241m.\u001b[39mappend(partial_labels)\n\u001b[1;32m    580\u001b[0m     filenames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m partial_filenames\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "training_set = image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "validation_set = image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_set = image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Get class names\n",
    "class_names = training_set.class_names\n",
    "\n",
    "# Initialize a counter for each class\n",
    "class_counts = Counter()\n",
    "\n",
    "for images, labels in training_set:\n",
    "    label_indices = np.argmax(\n",
    "        labels.numpy(), axis=1\n",
    "    )  # Convert one-hot to class indices\n",
    "    class_counts.update(label_indices)\n",
    "\n",
    "# Print class-wise image count\n",
    "for class_idx, count in class_counts.items():\n",
    "    print(f\"Class '{class_names[class_idx]}': {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Function to count images in each class\n",
    "# # def count_images_in_classes(dataset_dir):\n",
    "# #     \"\"\"\n",
    "# #     Counts the number of images in each class within a dataset directory.\n",
    "\n",
    "# #     Args:\n",
    "# #         dataset_dir (str): The path to the dataset directory.\n",
    "\n",
    "# #     Returns:\n",
    "# #         dict: A dictionary where the keys are the class names and the values are the number of images in each class.\n",
    "# #     \"\"\"\n",
    "# #     class_counts = {}\n",
    "# #     for class_name in os.listdir(dataset_dir):\n",
    "# #         class_path = os.path.join(dataset_dir, class_name)\n",
    "# #         if os.path.isdir(class_path):\n",
    "# #             class_counts[class_name] = len(os.listdir(class_path))\n",
    "# #     return class_counts\n",
    "\n",
    "\n",
    "# # # Count images in training and validation sets\n",
    "# # train_class_counts = count_images_in_classes(TRAIN_DIR)\n",
    "# # valid_class_counts = count_images_in_classes(VALID_DIR)\n",
    "\n",
    "# # Create a DataFrame for better visualization\n",
    "# df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Class\": list(training_set.class_names),\n",
    "#         \"Training Images\": list(training_set),\n",
    "#         \"Validation Images\": \n",
    "#             list(training_set)\n",
    "#         # [\n",
    "#             # valid_class_counts.get(cls, 0) for cls in train_class_counts.keys()\n",
    "#         # ],\n",
    "#     }\n",
    "# ).sort_values(by=\"Class\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the class distribution with adjustments for readability\n",
    "# df.plot(\n",
    "#     x=\"Class\", kind=\"bar\", stacked=True, figsize=(20, 8), title=\"Class Distribution\"\n",
    "# )\n",
    "# plt.ylabel(\"Number of Images\")\n",
    "# plt.xlabel(\"Class\")\n",
    "\n",
    "# # Rotate x-ticks for better readability\n",
    "# plt.xticks(rotation=90, ha=\"center\")\n",
    "\n",
    "# # Adjust layout to prevent clipping of labels\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to visualize one image per class\n",
    "# def visualize_sample_images(dataset_dir):\n",
    "#     \"\"\"\n",
    "#     Visualizes one sample image per class in the dataset directory.\n",
    "\n",
    "#     Args:\n",
    "#         dataset_dir (str): The path to the dataset directory.\n",
    "\n",
    "#     Displays a grid of images, with one image per class, using matplotlib.\n",
    "#     \"\"\"\n",
    "#     class_names = os.listdir(dataset_dir)\n",
    "#     class_names.sort()  # Sort for consistent order\n",
    "\n",
    "#     # Calculate the number of rows and columns for the subplot grid\n",
    "#     num_classes = len(class_names)\n",
    "#     num_cols = 5  # You can adjust this number\n",
    "#     num_rows = math.ceil(num_classes / num_cols)\n",
    "\n",
    "#     plt.figure(figsize=(num_cols * 3, num_rows * 3))\n",
    "#     for i, class_name in enumerate(class_names, start=1):\n",
    "#         class_path = os.path.join(dataset_dir, class_name)\n",
    "#         image_path = os.path.join(\n",
    "#             class_path, os.listdir(class_path)[0]\n",
    "#         )  # Get the first image in the class\n",
    "#         img = plt.imread(image_path)\n",
    "\n",
    "#         plt.subplot(num_rows, num_cols, i)  # Adjust grid size dynamically\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(class_name)\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize sample images from training set\n",
    "# visualize_sample_images(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Apply normalization on both Training and Validation set\n",
    "normalized_training_set = training_set.map(lambda x, y: (normalize(x), y))\n",
    "normalized_validation_set = validation_set.map(lambda x, y: (normalize(x), y))\n",
    "normalized_test_set = test_set.map(lambda x, y: (normalize(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brightness adjustment\n",
    "brighten = (\n",
    "    tf.keras.layers.RandomBrightness(\n",
    "        factor=(-0.1, 0.1),\n",
    "        value_range=(0.0, 1.0),\n",
    "    ),\n",
    ")  # Adjust brightness by ±20%\n",
    "\n",
    "# Contrast adjustment\n",
    "add_contrast = (tf.keras.layers.RandomContrast(factor=0.5),)  # Adjust contrast by ±20%\n",
    "\n",
    "# Rotation\n",
    "rotate = (\n",
    "    tf.keras.layers.RandomRotation(\n",
    "        factor=0.2,\n",
    "        fill_mode=\"constant\",\n",
    "        fill_value=0.0,\n",
    "    ),\n",
    ")  # Rotate by ±10% (36°)\n",
    "\n",
    "# Horizontal and vertical flips\n",
    "flip = (\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    ")  # Flip both horizontally and vertically\n",
    "\n",
    "# Zoom\n",
    "zoom = (\n",
    "    tf.keras.layers.RandomZoom(\n",
    "        height_factor=(-0.2, 0.2),\n",
    "        width_factor=(-0.2, 0.2),\n",
    "        fill_mode=\"constant\",\n",
    "        fill_value=0.0,\n",
    "    ),\n",
    ")  # Zoom in/out by 20%\n",
    "\n",
    "# Gaussian noise\n",
    "add_noise = (tf.keras.layers.GaussianNoise(stddev=0.01),)  # Add Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential([brighten, add_contrast, rotate, flip, zoom, add_noise])\n",
    "\n",
    "# Apply augmentation to the training set\n",
    "augmented_training_set = normalized_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Enhancement (not implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Enhancements have not strongly proven to increase the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Preprocessing Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Augmentation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the augmentation pipeline with individual augmentations\n",
    "def visualize_individual_augmentations(image_path):\n",
    "    \"\"\"\n",
    "    Visualizes the effect of individual augmentations on an input image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the input image.\n",
    "\n",
    "    Applies a series of individual augmentations to the input image and displays the results in a grid.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = load_img(image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))  # Adjust to your image size\n",
    "    image_array = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "    image_array = tf.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Define individual augmentation layers\n",
    "    augmentations = [\n",
    "        (\"Original\", None),\n",
    "        (\n",
    "            \"Random Brightness\",\n",
    "            brighten,\n",
    "        ),\n",
    "        (\"Random Contrast\", add_contrast),\n",
    "        (\n",
    "            \"Random Rotation\",\n",
    "            rotate,\n",
    "        ),\n",
    "        (\"Random Flip\", flip),\n",
    "        (\n",
    "            \"Random Zoom\",\n",
    "            zoom,\n",
    "        ),\n",
    "        (\"Gaussian Noise\", add_noise),\n",
    "    ]\n",
    "\n",
    "    # Apply each augmentation and plot\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i, (title, layer) in enumerate(augmentations, start=1):\n",
    "        if layer is None:\n",
    "            augmented_image = image_array[0]\n",
    "        else:\n",
    "            augmented_image = layer(image_array)[0]\n",
    "\n",
    "        plt.subplot(1, len(augmentations), i)\n",
    "        plt.imshow(augmented_image.numpy())\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual augmentation techniques\n",
    "visualize_individual_augmentations(SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the augmentation pipeline with a sample image\n",
    "def visualize_augmentation(image_path):\n",
    "    \"\"\"\n",
    "    Visualizes the effect of the augmentation pipeline on a sample image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the input image.\n",
    "\n",
    "    Applies the augmentation pipeline to the input image and displays the original image alongside 5 augmented versions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = load_img(\n",
    "        image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "    )  # Adjust to your image size\n",
    "    image_array = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "    image_array = tf.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Apply augmentations\n",
    "    augmented_images = [data_augmentation(image_array)[0] for _ in range(5)]\n",
    "\n",
    "    # Plot original and augmented images\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(image_array[0])\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for i, aug_img in enumerate(augmented_images, start=2):\n",
    "        plt.subplot(1, 6, i)\n",
    "        plt.imshow(aug_img.numpy())\n",
    "        plt.title(f\"Augmented {i-1}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Original vs Augmented Image\n",
    "visualize_augmentation(SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes in your project (38 classes)\n",
    "# num_classes = 72\n",
    "\n",
    "# Load InceptionV3 model pre-trained on ImageNet without the top (classification) layer\n",
    "base_model = EfficientNetV2B3(\n",
    "    weights=\"imagenet\", include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model (don't update weights during training)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(base_model.output)  # Reduce spatial dimensions\n",
    "x = BatchNormalization()(x)  # Normalize features to improve training stability\n",
    "x = Dense(512)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.4)(x)  # Dropout for regularization\n",
    "x = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)  # Add a smaller dense layer for hierarchical learning\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.25)(x)  # Another dropout layer with lower rate\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, activation=\"softmax\")(x)  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Callbacks for Early Stopping and Model Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"../models/checkpoints/{MODEL_NAME}_best_weights_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples in the training and validation datasets\n",
    "# train_samples = len(training_set)\n",
    "# validation_samples = len(\n",
    "#     validation_set\n",
    "# )\n",
    "\n",
    "# train_samples = sum([len(files) for _, _, files in os.walk(TRAIN_DIR)])\n",
    "# validation_samples = sum([len(files) for _, _, files in os.walk(VALID_DIR)])\n",
    "\n",
    "train_samples = sum(len(images) for images, _ in training_set)\n",
    "validation_samples = sum(len(images) for images, _ in validation_set)\n",
    "\n",
    "print(\"train_samples:\", train_samples)\n",
    "print(\"validation_samples:\", validation_samples)\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = (train_samples + (BATCH_SIZE - 1)) // BATCH_SIZE\n",
    "validation_steps = (validation_samples + (BATCH_SIZE - 1)) // BATCH_SIZE\n",
    "\n",
    "print(\"steps_per_epoch:\", steps_per_epoch)\n",
    "print(\"validation_steps:\", validation_steps)\n",
    "\n",
    "# Compute class weights to balance the dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(validation_set.class_names),\n",
    "    y=validation_set.class_names,\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_history = model.fit(\n",
    "    augmented_training_set.repeat(),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=normalized_validation_set.repeat(),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "fine_tuning_history = model.fit(\n",
    "    augmented_training_set.repeat(),\n",
    "    epochs=20,  # Additional fine-tuning epochs\n",
    "    validation_data=normalized_validation_set.repeat(),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set Accuracy\n",
    "train_loss, train_acc = model.evaluate(augmented_training_set)\n",
    "print(\"Training accuracy:\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set Accuracy\n",
    "val_loss, val_acc = model.evaluate(normalized_validation_set)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = (\n",
    "    BASE_DIR / \"models\" / f\"{MODEL_NAME}_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.keras\"\n",
    ")\n",
    "\n",
    "model.save(MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "TRAIN_HIS_DIR = BASE_DIR / \"training_histories\" / f\"training_history_{MODEL_NAME}_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.json\"\n",
    "FINE_TRAIN_HIS_DIR = BASE_DIR / \"training_histories\" / f\"fine_tunning_training_history_{MODEL_NAME}_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.json\"\n",
    "with open(\n",
    "    TRAIN_HIS_DIR,\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(training_history.history, f)\n",
    "\n",
    "with open(\n",
    "    FINE_TRAIN_HIS_DIR,\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(fine_tuning_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true labels\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_set], axis=0)\n",
    "\n",
    "if y_true.ndim > 1:  # If it's one-hot encoded\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "print(f\"y_true shape: {y_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the trained model\n",
    "y_pred = model.predict(normalized_test_set)\n",
    "\n",
    "if y_pred.ndim > 1:  # If it's one-hot encoded or probabilities\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(f\"y_pred shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred, target_names=test_set.class_names)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_heatmap(model, test_set, class_names):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix as a heatmap for a given model and validation dataset.\n",
    "    Uses human-readable class names for display.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model.\n",
    "        test_set: Test dataset (normalized).\n",
    "        class_names: List of class names.\n",
    "    \"\"\"\n",
    "    # Get true labels and predictions\n",
    "    true_labels = np.concatenate([y for x, y in test_set], axis=0)\n",
    "    predicted_probs = model.predict(test_set)\n",
    "\n",
    "    # If true_labels are one-hot encoded, convert them to class indices\n",
    "    if true_labels.ndim > 1:  # Check if one-hot encoded\n",
    "        true_labels = np.argmax(true_labels, axis=1)\n",
    "\n",
    "    # Convert predicted probabilities to class indices\n",
    "    predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plot confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        annot_kws={\"size\": 10},\n",
    "        cmap=\"magma\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Predicted Class\", fontsize=20)\n",
    "    plt.ylabel(\"Actual Class\", fontsize=20)\n",
    "    plt.title(\"Plant Disease Prediction Confusion Matrix\", fontsize=25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_heatmap(model, normalized_test_set, test_set.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train | Vaild Accuracy & Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracy and loss graphs.\n",
    "\n",
    "    Parameters:\n",
    "        history: The History object returned by model.fit().\n",
    "    \"\"\"\n",
    "    # Extract metrics\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs, val_acc, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graphs\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".AgroDiagnoseAI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
