{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning EfficientV2B3 Raw Data i224 b32 e30 ft2e20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the EfficientV2B3 model will be trained on the Raw Dataset. \n",
    "\n",
    "Image size is 224, Batch size is 32, Epochs is 30, model is trained then finetuned again on the same dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 22:57:44.762536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738364264.791173   33031 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738364264.802941   33031 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "from tensorflow.keras.applications import InceptionV3, EfficientNetV2B3\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "MODEL_NAME = \"tl_efficientv2b3_raw_i224_b32_e30_ft2e20\"\n",
    "\n",
    "NUM_CLASSES = 72\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "# Get the base directory\n",
    "BASE_DIR = Path.cwd().parent.parent\n",
    "\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /home/sam5io/sam_engineerings/AgroDiagnoseAI/datasets/plant_disease_dataset_noaugmentation_raw\n",
      "Sample Image Path: /home/sam5io/sam_engineerings/AgroDiagnoseAI/datasets/cropped_plant_village_dataset/sample_image.JPG\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = BASE_DIR / \"datasets\" / \"plant_disease_dataset_noaugmentation_raw\"\n",
    "SAMPLE_IMAGE = (\n",
    "    BASE_DIR / \"datasets\" / \"cropped_plant_village_dataset\" / \"sample_image.JPG\"\n",
    ")\n",
    "\n",
    "print(\"Directory:\", DATA_DIR)\n",
    "print(\"Sample Image Path:\", SAMPLE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116232 files belonging to 72 classes.\n",
      "Using 92986 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738364273.930313   33031 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116232 files belonging to 72 classes.\n",
      "Using 23246 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "training_set = image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "validation_set = image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116232 files belonging to 72 classes.\n",
      "Using 23246 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_set = image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Rose___slug_sawfly': 3954 images\n",
      "Class 'Soybean___healthy': 4030 images\n",
      "Class 'Squash___powdery_mildew': 1463 images\n",
      "Class 'Apple___healthy': 2032 images\n",
      "Class 'Tomato___bacterial_spot': 1738 images\n",
      "Class 'Orange___citrus_greening': 4385 images\n",
      "Class 'Grape___Leaf_blight': 1200 images\n",
      "Class 'Tomato___leaf_curl': 4276 images\n",
      "Class 'Cherry___healthy': 695 images\n",
      "Class 'Tomato___spider_mites': 1351 images\n",
      "Class 'Rice___bacterial_blight': 1278 images\n",
      "Class 'Corn___common_rust': 959 images\n",
      "Class 'Blueberry___healthy': 1177 images\n",
      "Class 'Cassava___bacterial_blight': 831 images\n",
      "Class 'Grape___black_rot': 1289 images\n",
      "Class 'Rose___rust': 3974 images\n",
      "Class 'Bell_pepper___healthy': 1199 images\n",
      "Class 'Tomato___late_blight': 1504 images\n",
      "Class 'Potato___early_blight': 2074 images\n",
      "Class 'Corn___northern_leaf_blight': 799 images\n",
      "Class 'Cassava___mosaic_disease': 10497 images\n",
      "Class 'Rose___healthy': 3973 images\n",
      "Class 'Rice___brown_spot': 1290 images\n",
      "Class 'Corn___healthy': 921 images\n",
      "Class 'Cassava___green_mottle': 1920 images\n",
      "Class 'Rice___tungro': 1065 images\n",
      "Class 'Apple___gray_spot': 318 images\n",
      "Class 'Grape___black_measles': 2146 images\n",
      "Class 'Cassava___healthy': 2032 images\n",
      "Class 'Cassava___brown_streak_disease': 1795 images\n",
      "Class 'Potato___healthy': 1830 images\n",
      "Class 'Tomato___leaf_mold': 777 images\n",
      "Class 'Peach___bacterial_spot': 1906 images\n",
      "Class 'Strawberry___leaf_scorch': 883 images\n",
      "Class 'Grape___healthy': 1352 images\n",
      "Class 'Cherry___powdery_mildew': 839 images\n",
      "Class 'Bell_pepper___bacterial_spot': 768 images\n",
      "Class 'Apple___scab': 975 images\n",
      "Class 'Watermelon___healthy': 165 images\n",
      "Class 'Potato___late_blight': 1688 images\n",
      "Class 'Sugercane___red_rot': 428 images\n",
      "Class 'Tomato___healthy': 1266 images\n",
      "Class 'Sugercane___yellow_leaf': 408 images\n",
      "Class 'Rice___blast': 1166 images\n",
      "Class 'Tomato___target_spot': 1103 images\n",
      "Class 'Tomato___early_blight': 793 images\n",
      "Class 'Sugercane___healthy': 404 images\n",
      "Class 'Potato___leafroll_virus': 410 images\n",
      "Class 'Potato___mosaic_virus': 539 images\n",
      "Class 'Strawberry___healthy': 377 images\n",
      "Class 'Apple___rust': 992 images\n",
      "Class 'Coffee___healthy': 634 images\n",
      "Class 'Raspberry___healthy': 307 images\n",
      "Class 'Coffee___red_spider_mite': 127 images\n",
      "Class 'Watermelon___mosaic_virus': 337 images\n",
      "Class 'Potato___pests': 495 images\n",
      "Class 'Potato___bacterial_wilt': 442 images\n",
      "Class 'Coffee___rust': 493 images\n",
      "Class 'Tomato___septoria_leaf_spot': 1439 images\n",
      "Class 'Sugercane___mosaic': 363 images\n",
      "Class 'Apple___black_rot': 492 images\n",
      "Class 'Peach___healthy': 290 images\n",
      "Class 'Apple___alternaria_leaf_spot': 215 images\n",
      "Class 'Tomato___mosaic_virus': 292 images\n",
      "Class 'Potato___spindle_tuber_viroid': 75 images\n",
      "Class 'Apple___brown_spot': 168 images\n",
      "Class 'Potato___phytophthora': 279 images\n",
      "Class 'Sugercane___rust': 411 images\n",
      "Class 'Corn___gray_leaf_spot': 412 images\n",
      "Class 'Watermelon___anthracnose': 121 images\n",
      "Class 'Potato___nematode': 57 images\n",
      "Class 'Watermelon___downy_mildew': 303 images\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Get class names\n",
    "class_names = training_set.class_names\n",
    "\n",
    "# Initialize a counter for each class\n",
    "class_counts = Counter()\n",
    "\n",
    "for images, labels in training_set:\n",
    "    label_indices = np.argmax(\n",
    "        labels.numpy(), axis=1\n",
    "    )  # Convert one-hot to class indices\n",
    "    class_counts.update(label_indices)\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Print class-wise image count\n",
    "for class_idx, count in class_counts.items():\n",
    "    print(f\"Class '{class_names[class_idx]}': {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Function to count images in each class\n",
    "# # def count_images_in_classes(dataset_dir):\n",
    "# #     \"\"\"\n",
    "# #     Counts the number of images in each class within a dataset directory.\n",
    "\n",
    "# #     Args:\n",
    "# #         dataset_dir (str): The path to the dataset directory.\n",
    "\n",
    "# #     Returns:\n",
    "# #         dict: A dictionary where the keys are the class names and the values are the number of images in each class.\n",
    "# #     \"\"\"\n",
    "# #     class_counts = {}\n",
    "# #     for class_name in os.listdir(dataset_dir):\n",
    "# #         class_path = os.path.join(dataset_dir, class_name)\n",
    "# #         if os.path.isdir(class_path):\n",
    "# #             class_counts[class_name] = len(os.listdir(class_path))\n",
    "# #     return class_counts\n",
    "\n",
    "\n",
    "# # # Count images in training and validation sets\n",
    "# # train_class_counts = count_images_in_classes(TRAIN_DIR)\n",
    "# # valid_class_counts = count_images_in_classes(VALID_DIR)\n",
    "\n",
    "# # Create a DataFrame for better visualization\n",
    "# df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Class\": list(training_set.class_names),\n",
    "#         \"Training Images\": list(training_set),\n",
    "#         \"Validation Images\": \n",
    "#             list(training_set)\n",
    "#         # [\n",
    "#             # valid_class_counts.get(cls, 0) for cls in train_class_counts.keys()\n",
    "#         # ],\n",
    "#     }\n",
    "# ).sort_values(by=\"Class\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the class distribution with adjustments for readability\n",
    "# df.plot(\n",
    "#     x=\"Class\", kind=\"bar\", stacked=True, figsize=(20, 8), title=\"Class Distribution\"\n",
    "# )\n",
    "# plt.ylabel(\"Number of Images\")\n",
    "# plt.xlabel(\"Class\")\n",
    "\n",
    "# # Rotate x-ticks for better readability\n",
    "# plt.xticks(rotation=90, ha=\"center\")\n",
    "\n",
    "# # Adjust layout to prevent clipping of labels\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to visualize one image per class\n",
    "# def visualize_sample_images(dataset_dir):\n",
    "#     \"\"\"\n",
    "#     Visualizes one sample image per class in the dataset directory.\n",
    "\n",
    "#     Args:\n",
    "#         dataset_dir (str): The path to the dataset directory.\n",
    "\n",
    "#     Displays a grid of images, with one image per class, using matplotlib.\n",
    "#     \"\"\"\n",
    "#     class_names = os.listdir(dataset_dir)\n",
    "#     class_names.sort()  # Sort for consistent order\n",
    "\n",
    "#     # Calculate the number of rows and columns for the subplot grid\n",
    "#     num_classes = len(class_names)\n",
    "#     num_cols = 5  # You can adjust this number\n",
    "#     num_rows = math.ceil(num_classes / num_cols)\n",
    "\n",
    "#     plt.figure(figsize=(num_cols * 3, num_rows * 3))\n",
    "#     for i, class_name in enumerate(class_names, start=1):\n",
    "#         class_path = os.path.join(dataset_dir, class_name)\n",
    "#         image_path = os.path.join(\n",
    "#             class_path, os.listdir(class_path)[0]\n",
    "#         )  # Get the first image in the class\n",
    "#         img = plt.imread(image_path)\n",
    "\n",
    "#         plt.subplot(num_rows, num_cols, i)  # Adjust grid size dynamically\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(class_name)\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize sample images from training set\n",
    "# visualize_sample_images(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Apply normalization on both Training and Validation set\n",
    "normalized_training_set = training_set.map(lambda x, y: (normalize(x), y))\n",
    "normalized_validation_set = validation_set.map(lambda x, y: (normalize(x), y))\n",
    "normalized_test_set = test_set.map(lambda x, y: (normalize(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to the training set\n",
    "augmented_training_set = normalized_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Enhancement (not implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Enhancements have not strongly proven to increase the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes in your project (38 classes)\n",
    "# num_classes = 72\n",
    "\n",
    "# Load InceptionV3 model pre-trained on ImageNet without the top (classification) layer\n",
    "base_model = EfficientNetV2B3(\n",
    "    weights=\"imagenet\", include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model (don't update weights during training)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(base_model.output)  # Reduce spatial dimensions\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# x = BatchNormalization()(x)  # Normalize features to improve training stability\n",
    "# x = Dense(512)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.4)(x)  # Dropout for regularization\n",
    "# x = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)  # Add a smaller dense layer for hierarchical learning\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.25)(x)  # Another dropout layer with lower rate\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, activation=\"softmax\")(x)  # Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Callbacks for Early Stopping and Model Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"../models/checkpoints/{MODEL_NAME}_best_weights_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_samples: 92986\n",
      "validation_samples: 23246\n",
      "steps_per_epoch: 2906\n",
      "validation_steps: 727\n",
      "Class weights: {0: np.float64(1.0), 1: np.float64(1.0), 2: np.float64(1.0), 3: np.float64(1.0), 4: np.float64(1.0), 5: np.float64(1.0), 6: np.float64(1.0), 7: np.float64(1.0), 8: np.float64(1.0), 9: np.float64(1.0), 10: np.float64(1.0), 11: np.float64(1.0), 12: np.float64(1.0), 13: np.float64(1.0), 14: np.float64(1.0), 15: np.float64(1.0), 16: np.float64(1.0), 17: np.float64(1.0), 18: np.float64(1.0), 19: np.float64(1.0), 20: np.float64(1.0), 21: np.float64(1.0), 22: np.float64(1.0), 23: np.float64(1.0), 24: np.float64(1.0), 25: np.float64(1.0), 26: np.float64(1.0), 27: np.float64(1.0), 28: np.float64(1.0), 29: np.float64(1.0), 30: np.float64(1.0), 31: np.float64(1.0), 32: np.float64(1.0), 33: np.float64(1.0), 34: np.float64(1.0), 35: np.float64(1.0), 36: np.float64(1.0), 37: np.float64(1.0), 38: np.float64(1.0), 39: np.float64(1.0), 40: np.float64(1.0), 41: np.float64(1.0), 42: np.float64(1.0), 43: np.float64(1.0), 44: np.float64(1.0), 45: np.float64(1.0), 46: np.float64(1.0), 47: np.float64(1.0), 48: np.float64(1.0), 49: np.float64(1.0), 50: np.float64(1.0), 51: np.float64(1.0), 52: np.float64(1.0), 53: np.float64(1.0), 54: np.float64(1.0), 55: np.float64(1.0), 56: np.float64(1.0), 57: np.float64(1.0), 58: np.float64(1.0), 59: np.float64(1.0), 60: np.float64(1.0), 61: np.float64(1.0), 62: np.float64(1.0), 63: np.float64(1.0), 64: np.float64(1.0), 65: np.float64(1.0), 66: np.float64(1.0), 67: np.float64(1.0), 68: np.float64(1.0), 69: np.float64(1.0), 70: np.float64(1.0), 71: np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Get the number of samples in the training and validation datasets\n",
    "# train_samples = len(training_set)\n",
    "# validation_samples = len(\n",
    "#     validation_set\n",
    "# )\n",
    "\n",
    "# train_samples = sum([len(files) for _, _, files in os.walk(TRAIN_DIR)])\n",
    "# validation_samples = sum([len(files) for _, _, files in os.walk(VALID_DIR)])\n",
    "\n",
    "train_samples = sum(len(images) for images, _ in training_set)\n",
    "validation_samples = sum(len(images) for images, _ in validation_set)\n",
    "\n",
    "print(\"train_samples:\", train_samples)\n",
    "print(\"validation_samples:\", validation_samples)\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = (train_samples + (BATCH_SIZE - 1)) // BATCH_SIZE\n",
    "validation_steps = (validation_samples + (BATCH_SIZE - 1)) // BATCH_SIZE\n",
    "\n",
    "print(\"steps_per_epoch:\", steps_per_epoch)\n",
    "print(\"validation_steps:\", validation_steps)\n",
    "\n",
    "# Compute class weights to balance the dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(validation_set.class_names),\n",
    "    y=validation_set.class_names,\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738364409.129403   33134 service.cc:148] XLA service 0x7fab14002ab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1738364409.130381   33134 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "I0000 00:00:1738364414.391365   33134 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-31 23:00:21.153693: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng55{k2=8,k13=1,k14=3,k18=1,k22=0,k23=0} for conv (f32[32,16,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,40,112,112]{3,2,1,0}, f32[16,40,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-01-31 23:00:21.462012: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.308405531s\n",
      "Trying algorithm eng55{k2=8,k13=1,k14=3,k18=1,k22=0,k23=0} for conv (f32[32,16,112,112]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,40,112,112]{3,2,1,0}, f32[16,40,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-01-31 23:00:23.535557: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng34{k2=0,k4=0,k5=1,k6=0,k7=0,k19=0} for conv (f32[32,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,16,113,113]{3,2,1,0}, f32[64,16,3,3]{3,2,1,0}), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-01-31 23:00:25.621938: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.086464994s\n",
      "Trying algorithm eng34{k2=0,k4=0,k5=1,k6=0,k7=0,k19=0} for conv (f32[32,64,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,16,113,113]{3,2,1,0}, f32[64,16,3,3]{3,2,1,0}), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-01-31 23:00:27.668962: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng55{k2=8,k13=1,k14=3,k18=1,k22=0,k23=0} for conv (f32[32,160,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,40,57,57]{3,2,1,0}, f32[160,40,3,3]{3,2,1,0}), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-01-31 23:00:29.355650: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.686793874s\n",
      "Trying algorithm eng55{k2=8,k13=1,k14=3,k18=1,k22=0,k23=0} for conv (f32[32,160,28,28]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,40,57,57]{3,2,1,0}, f32[160,40,3,3]{3,2,1,0}), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "I0000 00:00:1738364444.896071   33134 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1157 - loss: 3.8888\n",
      "Epoch 1: val_accuracy improved from -inf to 0.15947, saving model to ../models/checkpoints/tl_efficientv2b3_raw_i224_b32_e30_ft2e20_best_weights_2025_01_31_10_57_48_PM.keras\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 70ms/step - accuracy: 0.1157 - loss: 3.8887 - val_accuracy: 0.1595 - val_loss: 3.5142 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1454 - loss: 3.6262\n",
      "Epoch 2: val_accuracy improved from 0.15947 to 0.15998, saving model to ../models/checkpoints/tl_efficientv2b3_raw_i224_b32_e30_ft2e20_best_weights_2025_01_31_10_57_48_PM.keras\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 53ms/step - accuracy: 0.1454 - loss: 3.6261 - val_accuracy: 0.1600 - val_loss: 3.4514 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m2905/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1507 - loss: 3.5737\n",
      "Epoch 3: val_accuracy improved from 0.15998 to 0.17487, saving model to ../models/checkpoints/tl_efficientv2b3_raw_i224_b32_e30_ft2e20_best_weights_2025_01_31_10_57_48_PM.keras\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 53ms/step - accuracy: 0.1507 - loss: 3.5737 - val_accuracy: 0.1749 - val_loss: 3.3914 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1523 - loss: 3.5506\n",
      "Epoch 4: val_accuracy improved from 0.17487 to 0.17672, saving model to ../models/checkpoints/tl_efficientv2b3_raw_i224_b32_e30_ft2e20_best_weights_2025_01_31_10_57_48_PM.keras\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 54ms/step - accuracy: 0.1523 - loss: 3.5506 - val_accuracy: 0.1767 - val_loss: 3.3780 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m2905/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1554 - loss: 3.5308\n",
      "Epoch 5: val_accuracy did not improve from 0.17672\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 53ms/step - accuracy: 0.1554 - loss: 3.5308 - val_accuracy: 0.1682 - val_loss: 3.3913 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m2905/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1587 - loss: 3.5123\n",
      "Epoch 6: val_accuracy did not improve from 0.17672\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 53ms/step - accuracy: 0.1587 - loss: 3.5123 - val_accuracy: 0.1673 - val_loss: 3.3914 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m2905/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1616 - loss: 3.4900\n",
      "Epoch 7: val_accuracy improved from 0.17672 to 0.17990, saving model to ../models/checkpoints/tl_efficientv2b3_raw_i224_b32_e30_ft2e20_best_weights_2025_01_31_10_57_48_PM.keras\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 53ms/step - accuracy: 0.1616 - loss: 3.4900 - val_accuracy: 0.1799 - val_loss: 3.3248 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1649 - loss: 3.4756\n",
      "Epoch 8: val_accuracy did not improve from 0.17990\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 53ms/step - accuracy: 0.1649 - loss: 3.4756 - val_accuracy: 0.1796 - val_loss: 3.3170 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1664 - loss: 3.4650\n",
      "Epoch 9: val_accuracy improved from 0.17990 to 0.18881, saving model to ../models/checkpoints/tl_efficientv2b3_raw_i224_b32_e30_ft2e20_best_weights_2025_01_31_10_57_48_PM.keras\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 53ms/step - accuracy: 0.1664 - loss: 3.4650 - val_accuracy: 0.1888 - val_loss: 3.3036 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1656 - loss: 3.4627\n",
      "Epoch 10: val_accuracy did not improve from 0.18881\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 53ms/step - accuracy: 0.1656 - loss: 3.4627 - val_accuracy: 0.1824 - val_loss: 3.3035 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1674 - loss: 3.4565\n",
      "Epoch 11: val_accuracy did not improve from 0.18881\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m2906/2906\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 53ms/step - accuracy: 0.1674 - loss: 3.4565 - val_accuracy: 0.1782 - val_loss: 3.3094 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m 964/2906\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 43ms/step - accuracy: 0.1708 - loss: 3.4390"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_training_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalized_validation_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/sam_engineerings/AgroDiagnoseAI/.AgroDiagnoseAI_venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_history = model.fit(\n",
    "    augmented_training_set.repeat(),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=normalized_validation_set.repeat(),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "fine_tuning_history = model.fit(\n",
    "    augmented_training_set.repeat(),\n",
    "    epochs=20,  # Additional fine-tuning epochs\n",
    "    validation_data=normalized_validation_set.repeat(),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set Accuracy\n",
    "train_loss, train_acc = model.evaluate(augmented_training_set)\n",
    "print(\"Training accuracy:\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set Accuracy\n",
    "val_loss, val_acc = model.evaluate(normalized_validation_set)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = (\n",
    "    BASE_DIR / \"models\" / f\"{MODEL_NAME}_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.keras\"\n",
    ")\n",
    "\n",
    "model.save(MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history\n",
    "TRAIN_HIS_DIR = BASE_DIR / \"training_histories\" / f\"training_history_{MODEL_NAME}_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.json\"\n",
    "FINE_TRAIN_HIS_DIR = BASE_DIR / \"training_histories\" / f\"fine_tunning_training_history_{MODEL_NAME}_{now.strftime(\"%Y_%m_%d_%I_%M_%S_%p\")}.json\"\n",
    "with open(\n",
    "    TRAIN_HIS_DIR,\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(training_history.history, f)\n",
    "\n",
    "with open(\n",
    "    FINE_TRAIN_HIS_DIR,\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(fine_tuning_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true labels\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_set], axis=0)\n",
    "\n",
    "if y_true.ndim > 1:  # If it's one-hot encoded\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "print(f\"y_true shape: {y_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the trained model\n",
    "y_pred = model.predict(normalized_test_set)\n",
    "\n",
    "if y_pred.ndim > 1:  # If it's one-hot encoded or probabilities\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(f\"y_pred shape: {y_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred, target_names=test_set.class_names)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_heatmap(model, test_set, class_names):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix as a heatmap for a given model and validation dataset.\n",
    "    Uses human-readable class names for display.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained model.\n",
    "        test_set: Test dataset (normalized).\n",
    "        class_names: List of class names.\n",
    "    \"\"\"\n",
    "    # Get true labels and predictions\n",
    "    true_labels = np.concatenate([y for x, y in test_set], axis=0)\n",
    "    predicted_probs = model.predict(test_set)\n",
    "\n",
    "    # If true_labels are one-hot encoded, convert them to class indices\n",
    "    if true_labels.ndim > 1:  # Check if one-hot encoded\n",
    "        true_labels = np.argmax(true_labels, axis=1)\n",
    "\n",
    "    # Convert predicted probabilities to class indices\n",
    "    predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plot confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        annot_kws={\"size\": 10},\n",
    "        cmap=\"magma\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Predicted Class\", fontsize=20)\n",
    "    plt.ylabel(\"Actual Class\", fontsize=20)\n",
    "    plt.title(\"Plant Disease Prediction Confusion Matrix\", fontsize=25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_heatmap(model, normalized_test_set, test_set.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train | Vaild Accuracy & Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracy and loss graphs.\n",
    "\n",
    "    Parameters:\n",
    "        history: The History object returned by model.fit().\n",
    "    \"\"\"\n",
    "    # Extract metrics\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs, val_acc, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graphs\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".AgroDiagnoseAI_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
